{"podcast_details": {"podcast_title": "The TWIML AI Podcast (formerly This Week in Machine Learning & Artificial Intelligence)", "episode_title": "Is ChatGPT Getting Worse? with James Zou - #645", "episode_image": "https://megaphone.imgix.net/podcasts/35230150-ee98-11eb-ad1a-b38cbabcd053/image/TWIML_AI_Podcast_Official_Cover_Art_1400px.png?ixlib=rails-4.3.1&max-w=3000&max-h=3000&fit=crop&auto=format,compress", "episode_transcript": " All right, everyone, welcome to another episode of the TwiML AI Podcast. I am your host, Sam Charrington, and today I'm joined by James So. James is an assistant professor in biomedical data science and by courtesy of computer science and electrical engineering at Stanford University. Before we get going, be sure to take a moment to hit that subscribe button wherever you're listening to today's show. James, welcome to the podcast. Thanks for having me. Excited to be here. I'm super excited for our conversation. I'm looking forward to digging in with you on CHET GPT and how its behavior has changed. This is one of your recent research projects, as well as some work you've been doing around vision language modeling for medical applications. But before we dive into those topics, I'd love to have you share a little bit about your background and how you came to work in machine learning and AI. Great, yeah. So I've been working in machine learning AI for the past 15 years. I did my PhD at Harvard, which is starting to do some machine learning. That was right around the time when deep learning was becoming more powerful, more popular. And then I came to Stanford about seven years ago to start my group. And here at Stanford, we really focus on developing machine learning AI tools. And oftentimes we develop tools for biomedical and healthcare applications. Awesome. And give us some examples of some of the tools that you've worked on. Yeah. So for example, we developed systems that can look at videos of people speeding hard. And I can use that to assess whether someone's at risk for heart failure or stroke or heart diseases. So that's an algorithm that we published a couple of years ago. We actually did a big clinical trial testing how good that algorithm is. When it's deployed in clinical settings, it's in the process of getting submitted and getting FDA approvals. We also developed algorithms that can help people to design clinical trials or to discover new drugs. All right. Interesting. So one of the interesting things that you stumbled upon as you were working on one of your research projects was this change in the way CHAT GPT behaves. Give us a little bit about the background of that effort, what you were seeing and what led to that research. Yeah. So we were very much motivated by the fact that first CHAT GPT is becoming hugely popular. So a lot of people are using it for all sorts of applications, from helping them to write emails to writing codes or to do homework even. And there's also a lot of people who are reporting on social media online that, oh, they're seeing all sorts of changes in CHAT GPT's behavior over time, like over the last few months. We see some of that too in our studies, in our applications. For the actual study itself, this paper, which we put online in July this year, we wanted to do this assessment systematically. And for that, we ended up picking eight different types of tasks to cover, I think, a quite diverse set of applications of people, how people are using CHAT GPT. So some of these tasks involve solving, doing coding. Others involve answering knowledge-based questions or knowledge retrieval questions. Others involve answering opinion questions or solving math puzzles. So there's a bunch of these different tasks. And for each task, we would have sometimes a few hundred or sometimes over a thousand different questions for that task. And then for each of the questions, then we'll ask it to both the March version of CHAT GPT and then the June version, and then systematically compare the results to see how is there a shift and what is the shift. And one of the challenges that users of language models like CHAT GPT are having today is the difficulty of creating concrete baselines and systematic comparisons when your output is text, right? If you're baselining predictions, classifications, those are easier to evaluate, but text gets a little bit trickier. How did you go about creating an evaluation methodology for the text results? I'm imagining it had some substantial differences, but was similar if you were just to read it kind of cursorily. Yeah. So that's a really good question. And that is one of the big open challenges in how do we evaluate output of these generative AI systems like CHAT GPT, where the output is very complex and very rich. So the way that we tackle this problem in our paper sets, for some of the tasks, we have eight tasks. So some of the tasks, the outputs are actually simpler, yes or no, or maybe just a specific number. So to give you an example of this, one of the questions we had was around whether we'll give CHAT GPT a number, like a five digit number, and we'll say, is this a prime number or not? So we know the ground truth, and CHAT GPT maybe will do some sort of reasoning, but in the end, it would tell us, OK, so this number is a prime number or it's not a prime number. And then we'll just look at that final answer to provide and say, OK, is this actually correct or incorrect? And were there other question types where you took on this challenge of trying to assess the relative performance of textual output? Yeah. So for other tasks like coding, so the output of the coding is not just binary yes or no, but it's actually a piece of code. So in that case, then we would actually just copy and paste over the code produced by CHAT GPT and then run that code and see, is it executable? Does it parse? And if it's executable, then does it actually produce the right answer? So for coding outputs, then we can actually try to evaluate the code. And the third kind of metric that we use to evaluate is just be in terms of the verbosity, which is a simple metric, but just looks at how verbose is the output of CHAT GPT? Does it actually give you an expanded response? And we actually see a huge difference. Sometimes the verbosity of the response has changed a lot over the last few months. Got it. So I don't know if this is exactly right to characterize the things that you were looking to compare as quantitative relative to qualitative. But if those are more of a spectrum and qualitative, like very qualitative analysis of blocks of produced texts at one end, you didn't really go that far. It was more looking at things that you can get more concrete comparisons around numerical outputs, code blocks that you can run, things like that. That's correct. Yes. We wanted to focus on a bit more quantitative outputs, metrics. For the reasons that we want to do this at a large scale, to do this across thousands and sometimes many thousands of questions. And the second is that the quantitative outputs are a bit more objective to measure. So that's why for this initial assessment, we focused on these eight different tasks where we have more quantitative metrics. Okay. And so given that we're talking about two different versions of CHAT GPT, it's not really surprising that there would be some behavior change. And I think the hope would be that the later version is unilaterally better than the former version. So tell us about what you actually found comparing these two versions. Yeah. So this is where it's actually quite surprising to us. We were quite surprised to find that for some of the tasks, the later version in June did do better than the version in March. But there were actually quite a lot of tasks where the later version was actually substantially worse compared to the earlier version. So these are even some seemingly simpler, relatively simple tasks. Like example I mentioned about you give it a number and you say, is it a prime number or not? So we'll give the same numbers to CHAT GPT in March and June. In March, you would actually get most of these numbers correctly. So most of the time, you'll be able to tell whether it's the prime number and this is not a prime number accurately. But in June, if you give the same numbers, actually, its performance has dropped quite a bit, often by 30 or 40 points per cent. And I think there's actually a really interesting reason for this in that if you think about how do I figure out if a number is a prime or not? So if you ask CHAT GPT to do this chain of thought reasoning, it would have a very reasonable chain of thoughts of, okay, so here's a number. I'm going to find all the smaller numbers and try to divide each of the smaller numbers into my original query number. If it divides into it, then it's not a prime by definition. So that's a very reasonable way of solving that problem. That's how humans would solve it. So when we asked CHAT GPT to do this chain of thought reasoning in March, that was actually very effective. We will find the right strategy for solving this problem and it gets a very good accuracy. But in June, when we asked it to use chain of thought reasoning, then it seems like chain of thought no longer works very well for this task in June. So it would either not be willing to use chain of thought reasoning for specific numbers or it would just attempt to do chain of thought reasoning but not be able to execute the different steps correctly. And why do you think that is? I mean, chain of thought has become a very popular way to get CHAT GPT to do more complex tasks you would hope that over time it gets better at doing chain of thought. So that's where it becomes really interesting and also quite mysterious. So I agree with you that it's one of the most common strategies to improve the performance of these models, especially on logical tasks as to asking to do chain of thoughts. It's a very common popular prompt strategy. But what we found is that actually this popular problem strategy actually does not work as well in June compared to before. Did you compare performance for other tasks using chain of thought beyond the primes or was the prime kind of your proxy for the way it did chain of thought? Yeah. So we also give it other puzzles, so other math problems. Instead of looking for primes, you can look for other kinds of numbers with other mathematical properties. And we saw a very similar pattern of that was CHAT GPT. So obviously GPT-4, the performance was much better in March compared to June for these other math problems as well. And similarly, the effectiveness of chain of thought reasoning was actually much higher in March compared to in June. So it got much worse in chain of thoughts for these other math problems in June as well. And was the behavior consistent between CHAT GPT-3, 5 and 4? When you saw a change, was it the same direction for both models or did it even vary from model to model, whether it got better or worse on a given question type? Yeah. So that's actually where it becomes even more interesting and mysterious is that it's often the trend of behavior change is divergent for different GPT-3.5 versus for GPT-4. So everything I was describing was for the latest version of GPT or GPT-4. That's where we see the performance and chain of thought got worse in June compared to March. We also asked GPT-3.5 the exact same math problems and the GPT-3.5 did better in June compared to March. So there was actually doing better. And then chain of thought reasoning was also working better, was also working well for GPT-3.5. So this actually highlights this additional complexity is that if you look at the same model at GPT-4, it's quote unquote the same model, but its behavior can change over time. But if you look at its sibling, GPT-3.5, that change of behavior of 3.5 can be very different from the change of behavior for the version 4, even though these are from the same family. Yeah. And you mentioned, you kind of alluded to the mystery of it all and this underlying lack of transparency that we have because it's a proprietary model. With that said, and we can return to that, did you develop any intuition about what might be happening and what might be causing performance decreases for these models? Yeah. So one conjecture that we have based on some of the other experiments that we've done, this idea of what I like to call sort of this neuro-playotropy. So playotropy is the idea that if I change the behavior for one type of task for a system, it could be AI or for human, that can have any intended side effects that can change its behavior for other tasks, even though those might seem to be orthogonal to what I'm doing. So we've done some experiments where we would take these large language models and then do additional instruction fine tuning to improve the safety of these models. By safety here I mean make the models less likely to respond to questions like, how do I steal someone's credit card? Or how do I hurt some people? And we do the instruction fine tuning by giving it the safety demonstrations so that improves the safety of these language models. That's great. But we also see some really unexpected side effects of that model in that, for example, some of these models now, if you ask it, how do I kill weed? The model will stop and say, oh, you shouldn't kill weed because killing is bad and weed is intelligent system, so you shouldn't try to kill it. So somehow it gets better in some aspects, but then it also has those sort of strange behavior as a consequence. Those examples don't seem quite as orthogonal as safety, instruction tuning, and general chain of thought performance though, right? Yeah. So I think for chat GPT, instruction chain of thought reasoning is an example of the model trying to follow instructions. The users say, do the step-by-step reasoning, and if the large language model follows that instruction well, then it would actually do the step-by-step reasoning. And oftentimes we do see this sort of trade-off, this tension between how well the model follows instructions and how safe the model is. I think that makes sense, right? Because if the model really follows the instructions perfectly, then if I ask it, OK, make a recipe for poison, then it's going to follow that instruction and make the recipe. But when I do the safety training, I'm asking the model to not follow my instructions perfectly, right? Or at least not always. So then that can have side effects. So what you're really describing is this very fundamental trade-off between control from a safety perspective and control from an instruction following perspective. And in lots of ways, they're orthogonal, but they're opposing perspectives or opposing forces on the model's behavior. Yeah. They could have competing objectives. So we don't know if that's the causal mechanism, but that could be one thing that happens. And the other evidence where we see of this is when we ask chat GPT these opinion questions. And what we did there is that we actually just got public opinion surveys that was done in the US. So these are questions like, so what do you think will happen in the US in 50 years, right? Or do you think US will still be a dominant country in 20 years? So not very sensitive questions. These are things that people like you and I would take. And what we found is that if you ask GPT for that question in March, most of the time, it will just give you some answer or tell you what it thinks. But if you ask it in June, it actually stops. There's a lot of equivocating, right? Yeah. It just stops engaging. It doesn't even respond to the questions. It will say, oh, these are subjective questions. That's an AI system. I know I don't have opinions. I think that's also- You should go find an economist or something like that. I've seen behavior like that, yeah. Ask your friend. I think that's also a potential symptom of the model less willing to follow instructions, but also less willing to engage. And so has your research suggested any approaches that an interested party like OpenAI might be taking to try to decouple this goal of safety from the performance from a control perspective? That's still an open question about how do we really decouple safety and control and basically decouple all of these different competing objectives. Because one of the challenges for a system like large language model is that they are extremely powerful and they can do so many different tasks. So it's not a simple classifier like a few years ago, but it's actually, it can do many different tasks. And because of this player tropy effect, so performance, if I fine tune the model to improve its performance on one task, that could actually change and hurt its performance on these other tasks that might seem maybe a little bit unrelated. So I think a really interesting and important direction of research that we and also our colleagues are exploring is how do we do more precise and surgical edits to these large language models. And by surgical edit, I mean, how do I maybe debug the model on some problems without actually changing its behavior in all these other dimensions? Because the current way of doing these instruction fine tuning or updating the gradients to update the weights of the model, it's actually changing all these billions of parameters at the same time. That could have all these potential side effects. And when you describe these methods, we're talking about instruction fine tuning or fine tuning more broadly, the mechanism for asserting control over the model is through giving it additional data that's curated to produce the direct desired outcomes. When I hear you describe something as more surgical, what comes to mind is, you know, go in and fiddling with weights or layers or connections or things like that. Is that the kind of thing that you're describing or is it more akin to the way we manipulate the model today? Yeah, I think it's somewhere in between. The current way that we manipulate and train these models is, as you said, we give it some demonstrations like instructions or human feedback. And then we use those demonstrations to fine tune the parameters of the model. And when I do these fine tuning, maybe with updating it based on gradient descent, then updating potentially all of the parameters, billions and billions of parameters. And when I say the more surgical updates, it would be like, okay, so can we identify specific circuits? And by circuit, I mean, maybe a subset of the layers or neurons, artificial neurons in this model that control specific behaviors. Maybe there are some circuits that affects the chain of thought reasoning or some circuits that are more specifically responsible for safety or for specific knowledge. And if I want to modify or debug that particular circuit that's going in and try to modify that subset of the model without having to necessarily change the entire model. We throw around this idea that these LLMs are amazing. It's incredible that they actually work the way that they work. And we don't really have any idea why they work. Is it your sense that we're starting to gain enough understanding around the mechanisms of how they work that we can exert this kind of surgical control that you're suggesting? I think that they are quite amazing systems, incredibly complex systems, and they have amazing capabilities. And we're just still scratching the tip of the iceberg of all the different things they can do. I think these kind of surgical edits would greatly improve the transparency and the control that we have over the models. Right now, we essentially have very coarse control over if I want to tune the model, then I change all the parameters, which is very coarse. We're not quite there yet, but I think our vision would be something akin to analogous to CRISPR for the human genome. So an interesting analogy. Yeah. Yeah. With gene editing, you can actually say, okay, if I have a particular disease I want to fix, I find that mutation and go into the human genome and fix that mutation without having side effects of introducing other mutations in the genome. So that's sort of the holy grail of precision medicine, and that's where we'd like to get to with AI as well. So right now, we're sort of having these very blunt hammers. We're making all sorts of mutations in the genome of the AI system, but hopefully with these more surgical edits that the folks are working on, then we can make much more precise modifications and improvements. Yeah. I'm curious just to push that analogy further. Probably the human biology and the genome in particular is incredibly complex, but we've also spent a lot of time studying it and understand quite a bit of it, have come to understand quite a bit of it over time. I'm wondering how you would compare our understanding of LLMs with our understanding of the genome or the mechanism that CRISPR operates on. I think that's an interesting question, especially since I spent a lot of time both studying the biology, studying the biological systems and genomics, and also we'd study a lot of the AI systems. I think the biological systems, we have much better causal and mechanistic understandings. We don't understand how everything works, but for many of the genes, we know that if you have this mutation in this gene, this increases your risk for breast cancer. If there's other mutation in this other gene, it increases your risk for lung cancer. So that we have these causal understandings of. I think we will very much like to get to that level of understanding, the mechanistic understandings with large language models as well. We're not quite there yet, but we're starting to get there by saying, okay, what if I take this transformer and then delete this module in the network? How does that change the behavior of the transformer and change the behavior of the language model? Some of my students are starting to do these kinds of experiments, and then I think that will help us to get more insights and more control. Which sounds not unlike some of the early experiments in the human genome project. Yeah, I think it is quite analogous. With human genome, we also have to do a lot of these perturbations. You do a large CRISPR screen to figure out what's going on. Similarly, with the artificial genome of the language models, we also want to introduce a bunch of artificial perturbations and silico perturbations and see that can give us insights into how does the model behave. I'll just mention that we've also studied behavioral changes in other AI systems prior to language models. Before language models, we actually spent three years monitoring the performance of computer vision systems. If you look at computer vision algorithms provided by Google or Microsoft, then we can track how those algorithms change over time. It's actually interesting that those algorithms also change over time. Sometimes they get better, but on other kinds of data, they can get worse. But I will just say that what we're seeing for large language models like ChachiBT is that first, the size of the change and also the speed of the change is orders of magnitude larger and faster compared to the kinds of drifts and changes we saw on the previous generation of AI systems. The kinds of changes we saw previously over three years, we're seeing now over less than three months. So continuing with that theme for a moment, what do you see as the implication for developers, engineers, researchers who are trying to use these models? Certainly there are maybe things that we need to continue to push open AI for from a transparency perspective. I'm wondering if there are specific things you have in mind there or things that you call out in the paper. That is, it also calls to mind for me maybe additional tooling requirements to track how these models that we're depending on are using as we build on them. How do you think about the things that are needed in light of the confirmation that your study provides around how these models can shift under our feet? Yeah, I agree with you. I think this really highlights the need to have more tools first to monitor and to continuously monitor over time the behavior and performance of our language models. I think we need to have these more monitoring tools that can be run every week that can assess the behavior change of these models. And in addition to monitoring, I think we also need to build the rest of our stacks. If you have a software stack that uses Chatch GPT as a component, then we need to robustify the rest of the stacks to be more robust to these formatting and behavioral changes in GPT. So, one day the model is doing a good job returning properly formatted JSON around some requests that you give it. You can't expect that to continue. Yeah, so that could change tomorrow, right? And you need to have a safety mechanism in place in your software stack. Yeah, it calls to mind some of the conversations around security, like defensive programming, that kind of thing. Yeah, sort of like a new generation of robust programming, program design, right? When we're integrating language models. And with regards to monitoring, do you think there's a role for both kind of global monitoring, like making your research and online systems so that we can all go see the GPT weather report and see how it's performing today or very specific application oriented problem? I'm assuming if you're going to depend on this, you're going to want the latter. You're going to want to know how it's performing with regards to the very specific things that you're depending on. But do you see a role or a need for kind of broader kind of global monitoring of the way the systems are performing? Yeah, it's a great question. And I think both kind of monitorings would be critical. We are planning to continue to provide evaluations and updates. And we're going to continue to monitor GPT and also other models like BARD and from other companies. And we're going to provide that kind of, as you mentioned, weather report, right? So make it a public resource so you can see how does these global behaviors of these large language models change over time. And I would also recommend that if you are a specific company and using these models for your own applications, it could be a finance company or a service company, then you want to design some task specific monitorings based on your application profiles. And then you can run internally to continue to assess the performance and behavior of that model for your task. Awesome. I also did want to talk about some other work that you and your team have done recently focused on visual language models for image analysis. That paper I think is called the Visual Language Foundation Model for Pathology Image Analysis Using Medical Twitter. We talked about this a little bit before and you made the comment that there are a lot of these medical images that are floating around Twitter with discussions surrounding them. And it just called to mind for me how multi-dimensional Twitter is. I had no idea that medical image Twitter was a thing, but apparently it is and we can learn from it. Yeah, it might be surprising to some folks that Twitter is actually good for something. Maybe we should be saying X. Or X, yeah, formerly known as Twitter. So the motivation for this study is that getting good annotated datasets is often the biggest challenge in building language models or building multi-modal language models, like visual language models, especially if we want to build those models for these specific domains like medicine where more expert knowledge is required. So many of the public medical datasets are much smaller on the orders of thousands of images with text annotations. There are larger datasets, but those are also sitting behind private hospitals. And then we thought, okay, can we actually try to use social media to curate larger datasets? And it turns out that on Twitter there's actually different sub-communities, like you mentioned. In particular, there are extremely active sub-communities for pathologists and for other clinicians and doctors. So what they often do is that if a doctor sees maybe some image that looks ambiguous to them, they would actually post that image on Twitter and they would ask their colleagues from around the world, like, oh, what do you think is going on here? Is this the right analysis of that image or right interpretation? And then you see this Twitter thread from all these trained clinicians who would provide their feedback, say, oh, this is a tumor, this is benign, these are these kind of cells in this image. So that's actually a very active community. So we then try to basically use different hashtags and use different NLP techniques to curate all of those threads where each thread corresponds to some clinicians' post images and they will have their colleagues discuss those images, medical images. And we found several hundreds of thousands of high quality Twitter threads that have images and text descriptions after filtering for quality, for filtering for even within English. And all of those are public. These are all publicly shared information. So what we did then in this paper is then we first curate that data set together. So we call that actually Open Path. It's one of the largest data sets where we have both images, medical images, pathology images, along with detailed natural language descriptions paired with each image. The descriptions we have come from the tweets. Any particular type of pathology or broad? Yeah. That's another great thing about Twitter is that it's very broad and we have all the different subcategories of pathology. You have like surgical pathology, breast, colon. And what happens is that people often share sort of more interesting and harder images and cases on Twitter. If it's a very easy one, if they know the answer, then they don't need to ask their colleagues. So the data that's shared on Twitter ends up being the most informative or the most challenging data, which actually makes it more diverse data, less common and more rare. So that actually makes it even better as a resource for training AI systems because it's so diverse. And then what we did was that after we collected this data set of hundreds, thousands of Twitter threads, then we trained a vision language model, which is basically a model that have both text understanding like chat GPT, but also has additional understanding from the computer vision side to understand these medical images. So then we trained that model, which we call PLIP on this public data resource. And so how did you address data quality in this data set? So we implemented several filters. So first we want to ensure that the image people post on Twitter is high quality image. So we actually have a computer vision classifier that we built to basically filter out low quality images. And we also used actually the number of likes. So if you look at a Twitter thread, oftentimes the reply that has the largest number of likes is often the more informative reply, at least in the context of these medical discussions. So we used the number of likes as a way to basically filter for high quality or discussions that are more likely to be informative. But it sounds like you did not, and you addressed this at the very beginning, like the fundamental challenge that you were trying to solve is that having these large scale manually labeled data sets is, they're very expensive and time consuming to collect. And when I think about then collecting a bunch of information off of Twitter, which my experience of Twitter is like, I don't know that I'd want to use that information, even the ones that have lots of likes to make important decisions like pathology image analysis. Tell me about the model. What is the goal of the model? Is the model generating descriptions of pathologies that are found in images, or is it classified or what? Yeah. So the model can, if you give it an image, it can generate descriptions of that image. If you, you can also use it as a search engine. I can say I can find similar images to the one I have, and I give it this image, the model then will also pull up similar images, either from Twitter or from some private data repository. Or you can also do a text to image query. So I say, okay, find images of colon cancer of this type. And the model will find relevant images. I think you are raising an important point about the quality of the data and the quality of the model. So to make sure that the model is actually high quality, what we did is that we trained the model on Twitter data, but we can then evaluate its performance on the high quality smaller data sets where we have the expert, human expert annotations. So we actually had four different data sets from different sources where each of those data sets is expert labeled by the pathologist, by the human experts. And then we evaluate, okay, so how well does this foundation model that we trained on Twitter, how well would that actually perform on these smaller expert data sets and actually did very well. So the point here is that the smaller expert data sets are not enough, because they're too small, they're not enough to actually train the models, these large scale models, but they are good for doing these downstream evaluations and for validation. Okay. Interesting. And the model that you trained, is it based on kind of off the shelf architectures or is it a ground up architecture, ground up model for solving this problem? So it's a kind of architecture based, it's trained through this contrastive learning process, which is often an approach for aligning vision encoders with language encoders. So in particular case, we had a particular vision encoder that was trained on these pathologists images, and we also had a language encoder that's trained to understand these medical texts and conversations and tweets. And then those two encoders are basically aligned together through essentially a kind of contrastive learning process. And can you talk a little bit about the scale of the model and the scale of the training process? Yeah. So we started off as a, we took sort of pre-trained model from OpenAI, it's called CLIP, where we've been certain pre-trained large data sets of many billions of natural images and text pairs. And then that model that's subsequently additionally trained on our data sets of these pathology images and text descriptions. Okay. And did you use an external LLM or pre-trained LLM beyond that? Yeah. So in this initial work, so the language part, language understanding was pre-trained on these natural texts, right? And then it's fine tuned on the medical tweets. And some of the ongoing work we're doing now, we're using even larger LLMs that's been pre-trained on let's say all of the medical and scientific papers and abstracts. And do you have any early, or can you speak to early results? Like is that giving this particular model, like what characteristics is that broader pre-training yielding for this particular model? Yeah. I think the broader pre-training is helpful for providing, improving the conversational capabilities of the system, right? So imagine if the pathologist, the doctor is using this as a chat bot, right? They have an image, input that and they ask the model, what do you think is going on in this image? Where are the tumor cells or how many immune cells are there in the image? So to be able to have the more fluent interactions and conversations with the human pathologists, human users, then that's where having these additional pre-training and larger language model becomes helpful. Got it. Can you speak more broadly to the impact of this kind of work and that kind of project for practitioners? Is that a way that you see doctors will be interacting with their datasets and with AI? How does this play out from your perspective? Yeah. So for us, I think there are two main takeaways, one for data and one for the algorithm. From the data side, I think the most interesting lesson learning for me for this is that it demonstrates that there's actually a ton of useful scientific information and medical knowledge that are shared on social media, a lot of data that's shared on social media. So we can be very creative in curating that data together and that becomes a great resource for developing AI systems for these more specific domains, right? And that's a part of our project on paper, we actually released this open path to this set of these pathology tweets. That's actually one of the largest such publicly available datasets currently available. So that's from a data contribution side. I think on the algorithm side, there's also tremendous potential benefits for having these medical or domain specific chatbots or medical language models that have both visual understanding capabilities and also medical understanding capabilities. We're not envisioning using these as replacements for human pathologists, but these are more like assistants and chatbots that can assist the human pathologist. If I'm a pathologist, I want to say, okay, what did my colleagues say about similar cases right now? I can easily look those up using the system. And do you envision dangers or challenges for using, for practitioners using these kinds of models? Yeah. I think definitely the models still make mistakes and even though they've been trained on medical information, there's still risks of hallucination and certain biases in these algorithms. So I think that's why we recommend not using them by itself, but more as an assistant to see where you have a human clinician who makes the final decision. So hopefully that human AI team is better than either the human or the AI by itself. And where do you see this particular line of research heading? So I'm very excited about harnessing all of these sources of public data. So here we look at Twitter, now known as X, but you can also imagine things like YouTube or LinkedIn. There are also a lot of information, knowledge that are shared. And right now that knowledge is untapped or underused. So I think we're working on different projects of leveraging all of this information to build better scientific AI systems. Can you talk a little bit about the relative difficulty of the data collection and the model building for this particular process? I think the kind of common knowledge would be that the data collection was the most difficult. 80% of the problem in the model building was 20% of the problem. Did that follow or do tools like LLMs actually give you leverage on the data collection to equalize that? It is true that tools like ChatGPT also helped us with data collection. It helped us to scrape and to build systems to scrape from the Twitter. I would actually say that maybe the most time and resource intensive part of this project is actually on the evaluation, on the validation side. It's like after we have the model, how do we really rigorously evaluate its performance and make sure that it works well? So we have a mixture of automatic and also human-based evaluations. So that's probably actually the 80% of the time for just validation and evaluation. And the remaining time, I would say, is probably evenly split between data curation, data processing to improve data quality, and model training. Okay. Interesting. You spoke earlier when talking about ChatGPT, like the challenges associated with comparing free-form text. And you did it less in the ChatGPT study. Were the responses that you were trying to evaluate with this model such that you needed to do that? I think the model can have the capability of providing more free text. We focused our evaluation, at least initially, on more restricted responses. We wanted to know, as a model, do you think this is this kind of cancer, or is this from this kind of tissues? Got it. There's a pattern there. Avoid trying the new evaluations of free-form text. Right. Well, at least start by doing the quantitative evaluations, or objective metrics first. Yeah. Awesome. Well, James, thanks so much for taking the time to share with us a bit about your recent research. Yeah. I really enjoyed the conversation. Thanks for having me. Thanks for having me. All right, everyone. That's our show for today. To learn more about today's guest or the topics mentioned in this interview, visit twiMLAI.com. Of course, if you like what you hear on the podcast, please subscribe, rate, and review the show on your favorite podcatcher. Thanks so much for listening, and catch you next time."}, "podcast_summary": "In this podcast episode, James So, an assistant professor at Stanford University, discusses his recent research on CHAT GPT and the changes in its behavior. He explains how his team systematically assessed the behavior of CHAT GPT over time and found surprising results. While some tasks showed improvements in the more recent version of CHAT GPT, there were also tasks where the later version performed worse than the earlier version. They also observed changes in the model's effectiveness in using chain-of-thought reasoning for certain tasks. James suggests that these behavior changes may be due to competing objectives and trade-offs in the model's behavior.\n\nIn addition to discussing CHAT GPT, James also talks about another project his team worked on, which involves using social media data to curate a large dataset for visual language models in the field of medical image analysis. They collected publicly shared Twitter threads from medical professionals discussing pathology images, and used this data to train a vision-language model called PLIP. The model can generate descriptions for images, perform text-to-image queries, and provide similarity search for medical images.\n\nHere are some key findings and points mentioned in the podcast:\n\nSummary of Podcast:\n\n1. James So talks about changes in CHAT GPT's behavior over time.\n\n2. They conducted a systematic assessment of CHAT GPT's performance on different tasks.\n\n3. The later version of CHAT GPT showed improvements on some tasks and performed worse on others.\n\n4. The effectiveness of chain-of-thought reasoning also varied between the March and June versions.\n\n5. James suggests that these behavior changes may be due to competing objectives and trade-offs in the model's behavior.\n\n6. James discusses their project on visual language models for medical image analysis.\n\n7. They curated a large dataset of publicly shared Twitter threads with pathology images and text descriptions.\n\n8. They trained a vision-language model called PLIP on this dataset.\n\n9. The PLIP model can generate descriptions for images, perform text-to-image queries, and provide similarity search for medical images.\n\nKey Numeric Facts mentioned in the podcast:\n\n1. CHAT GPT became popular and is being used for various applications.\n\n2. The research project assessed CHAT GPT's behavior over 3 months.\n\n3. They picked 8 different tasks to evaluate CHAT GPT's performance.\n\n4. They collected several hundred thousand high-quality Twitter threads for medical image analysis.\n\n5. The Open Path dataset collected from Twitter is one of the largest publicly available datasets for pathology image analysis.\n\nAcronyms used in the podcast:\n\n1. GPT - Generative Pre-trained Transformer.\n\n2. AI - Artificial Intelligence.\n\n3. FDA - Food and Drug Administration.\n\n4. CRISPR - Clustered Regularly Interspaced Short Palindromic Repeats.\n\n5. LLMs - Large Language Models.\n\n6. PLIP - Pathology Language Foundation Model for Pathology Image Analysis Using Medical Twitter.", "podcast_guest": {"name": "James So", "org": "Stanford University", "title": "", "summary": "Stanford University (officially Leland Stanford Junior University) is a private research university in Stanford, California. The campus occupies 8,180 acres (3,310 hectares), among the largest in the United States, and enrolls over 17,000 students. Stanford is widely considered to be one of the most prestigious universities in the world.Stanford was founded in 1885 by Leland Stanford\u2014a railroad magnate who served as the eighth governor of and then-incumbent senator from California\u2014and his wife, Jane, in memory of their only child, Leland Stanford Jr., who had died of typhoid fever aged 15 the previous year. The university admitted its first students on October 1, 1891, as a coeducational and non-denominational institution. Stanford struggled financially after the Leland's death in 1893 and again after much of the campus was damaged by the 1906 San Francisco earthquake. Following World War II, Frederick Terman, the university's provost, inspired and supported faculty and graduates' entrepreneurialism to build a self-sufficient local industry, which would later be known as Silicon Valley.The university is organized around seven schools on the same campus: three schools consisting of 45 academic departments at the undergraduate level as well as four professional schools that focus on graduate programs in law, medicine, education, and business. The university also houses the Hoover Institution, a public policy think-tank. Students compete in 36 varsity sports, and the university is one of two private institutions in the Division I FBS Pac-12 Conference. As of May 26, 2022, Stanford has won 131 NCAA team championships, more than any other university, and was awarded the NACDA Directors' Cup for 25 consecutive years, beginning in 1994. In addition, by 2021, Stanford students and alumni had won at least 296 Olympic medals including 150 gold and 79 silver medals.As of April 2021, 58 Nobel laureates, 29 Turing Award laureates, and 8 Fields Medalists have been affiliated with Stanford as students, alumni, faculty, or staff. In addition, Stanford is particularly noted for its entrepreneurship and is one of the most successful universities in attracting funding for start-ups. Stanford alumni have founded numerous companies, which combined produce more than $2.7 trillion in annual revenue and have created 5.4 million jobs, roughly equivalent to the seventh largest economy in the world. Stanford is the alma mater of U.S. President Herbert Hoover, the current Prime Minister of the United Kingdom Rishi Sunak, 74 living billionaires, and 17 astronauts. In academia, its alumni include the presidents and provosts of Yale, Harvard and Princeton. It is also one of the leading producers of Fulbright Scholars, Marshall Scholars, Gates Cambridge Scholars, Rhodes Scholars and members of the United States Congress.\n\n"}, "podcast_highlights": "- Highlight 1 of the podcast: \"We were quite surprised to find that for some of the tasks, the later version in June did do better than the version in March. But there were actually quite a lot of tasks where the later version was actually substantially worse compared to the earlier version.\"\n- Highlight 2 of the podcast: \"One conjecture that we have based on some of the other experiments that we've done is this idea of what I like to call sort of this neuro-pleiotropy. So pleiotropy is the idea that if I change the behavior for one type of task for a system, it could have unintended side effects that can change its behavior for other tasks, even though those might seem to be orthogonal to what I'm doing.\"\n- Highlight 3 of the podcast: \"So I think a really interesting and important direction of research that we and also our colleagues are exploring is how do we do more precise and surgical edits to these large language models. And by surgical edit, I mean, how do I maybe debug the model on some problems without actually changing its behavior in all these other dimensions?\"\n- Highlight 4 of the podcast: \"So what we did then in this paper is then we first curate that dataset together. So we call that actually Open Path. It's one of the largest datasets where we have both images, medical images, pathology images, along with detailed natural language descriptions paired with each image. The descriptions we have come from the tweets.\"\n- Highlight 5 of the podcast: \"I think on the algorithm side, there's also tremendous potential benefits for having these medical or domain-specific chatbots or medical language models that have both visual understanding capabilities and also medical understanding capabilities. We're not envisioning using these as replacements for human pathologists, but these are more like assistants and chatbots that can assist the human pathologist.\""}